# Logstash configuration for SVG-AI log processing

input {
  # Beats input for Filebeat logs
  beats {
    port => 5044
    host => "0.0.0.0"
  }

  # Kubernetes logs via HTTP
  http {
    port => 8080
    host => "0.0.0.0"
    codec => json
    additional_codecs => {
      "application/json" => "json"
    }
  }

  # Syslog input
  syslog {
    port => 5140
    host => "0.0.0.0"
  }

  # Redis input for queued logs
  redis {
    host => "redis-service"
    port => 6379
    password => "${REDIS_PASSWORD}"
    data_type => "list"
    key => "svg-ai-logs"
    codec => json
  }
}

filter {
  # Parse Kubernetes logs
  if [kubernetes] {
    # Add cluster information
    mutate {
      add_field => { "cluster" => "svg-ai-prod" }
      add_field => { "environment" => "production" }
    }

    # Parse container logs
    if [kubernetes][container][name] == "svg-ai-api" {
      # Parse API logs
      grok {
        match => {
          "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:log_message}"
        }
      }

      # Extract request information for API logs
      if [log_message] =~ /HTTP/ {
        grok {
          match => {
            "log_message" => "HTTP %{WORD:method} %{URIPATH:path} %{INT:status_code} %{NUMBER:response_time}ms"
          }
        }
        mutate {
          convert => { "status_code" => "integer" }
          convert => { "response_time" => "float" }
        }
      }
    }

    if [kubernetes][container][name] == "svg-ai-worker" {
      # Parse worker logs
      grok {
        match => {
          "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:log_message}"
        }
      }

      # Extract optimization metrics
      if [log_message] =~ /optimization/ {
        grok {
          match => {
            "log_message" => "Optimization %{WORD:status} for %{DATA:image_id} in %{NUMBER:duration}s with SSIM %{NUMBER:ssim}"
          }
        }
        mutate {
          convert => { "duration" => "float" }
          convert => { "ssim" => "float" }
        }
      }
    }

    # Parse PostgreSQL logs
    if [kubernetes][container][name] == "postgres" {
      grok {
        match => {
          "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{INT:pid}\] %{LOGLEVEL:level}:  %{GREEDYDATA:log_message}"
        }
      }

      # Extract slow query information
      if [log_message] =~ /duration/ {
        grok {
          match => {
            "log_message" => "duration: %{NUMBER:query_duration} ms  statement: %{GREEDYDATA:query}"
          }
        }
        mutate {
          convert => { "query_duration" => "float" }
        }
      }
    }

    # Parse Redis logs
    if [kubernetes][container][name] == "redis" {
      grok {
        match => {
          "message" => "%{INT:pid}:%{CHAR:role} %{TIMESTAMP_ISO8601:timestamp} %{CHAR:level} %{GREEDYDATA:log_message}"
        }
      }
    }
  }

  # Parse application metrics
  if [fields][service] == "svg-ai" {
    # Add service tags
    mutate {
      add_tag => ["svg-ai", "application"]
    }

    # Parse custom metrics
    if [message] =~ /METRIC/ {
      grok {
        match => {
          "message" => "METRIC %{WORD:metric_name}=%{NUMBER:metric_value} %{GREEDYDATA:metric_tags}"
        }
      }
      mutate {
        convert => { "metric_value" => "float" }
      }
    }
  }

  # Geoip for external requests
  if [client_ip] and [client_ip] !~ /^10\./ and [client_ip] !~ /^172\./ and [client_ip] !~ /^192\.168\./ {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }

  # Parse user agent
  if [user_agent] {
    useragent {
      source => "user_agent"
      target => "ua"
    }
  }

  # Add processing timestamp
  mutate {
    add_field => { "processed_at" => "%{@timestamp}" }
  }

  # Remove unwanted fields
  mutate {
    remove_field => ["agent", "ecs", "host", "input"]
  }

  # Convert timestamp
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }
}

output {
  # Main Elasticsearch output
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    user => "${ELASTICSEARCH_USERNAME}"
    password => "${ELASTICSEARCH_PASSWORD}"

    # Dynamic index based on service
    index => "svg-ai-%{[kubernetes][container][name]:other}-%{+YYYY.MM.dd}"

    # Document type
    document_type => "_doc"

    # Template management
    manage_template => true
    template_name => "svg-ai-logs"
    template_pattern => "svg-ai-*"
    template => "/usr/share/logstash/templates/svg-ai-template.json"
  }

  # Error handling - send failed events to dead letter queue
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    user => "${ELASTICSEARCH_USERNAME}"
    password => "${ELASTICSEARCH_PASSWORD}"
    index => "svg-ai-errors-%{+YYYY.MM.dd}"
    document_type => "_doc"

    # Only for events that failed processing
    if "_grokparsefailure" in [tags] or "_dateparsefailure" in [tags] {
    }
  }

  # Debug output (comment out in production)
  # stdout { codec => rubydebug }

  # Metrics output to Prometheus
  if [metric_name] {
    http {
      url => "http://prometheus-pushgateway:9091/metrics/job/logstash/instance/svg-ai-logstash"
      http_method => "post"
      format => "message"
      message => "%{metric_name} %{metric_value}"
    }
  }
}